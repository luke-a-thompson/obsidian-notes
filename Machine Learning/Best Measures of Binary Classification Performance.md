1. **Accuracy**: The proportion of correct predictions made by the model.
2. **Precision**: The proportion of true positives (correctly predicted positive instances) among all positive predictions.
3. **Recall**: The proportion of true positives among all actual positive instances.
4. **F1 score**: The harmonic mean of precision and recall.
5. **ROC AUC score**: The area under the receiver operating characteristic (ROC) curve, which plots the true positive rate against the false positive rate at various threshold settings.
6. **Log loss**: The logarithm of the likelihood function, which measures the performance of a classifier where the predicted output is a probability value between 0 and 1.
7. **Matthews correlation coefficient (MCC)**: A correlation coefficient between the observed and predicted binary classifications.
